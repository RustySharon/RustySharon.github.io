<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://rustysharon.github.io</id>
    <title>沙子豪</title>
    <updated>2021-08-23T03:17:17.735Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://rustysharon.github.io"/>
    <link rel="self" href="https://rustysharon.github.io/atom.xml"/>
    <subtitle>许求聪慧者，童稚捧应癫。</subtitle>
    <logo>https://rustysharon.github.io/images/avatar.png</logo>
    <icon>https://rustysharon.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, 沙子豪</rights>
    <entry>
        <title type="html"><![CDATA[大厂面试总结高频题]]></title>
        <id>https://rustysharon.github.io/post/da-han-mian-shi-40-wen/</id>
        <link href="https://rustysharon.github.io/post/da-han-mian-shi-40-wen/">
        </link>
        <updated>2021-08-23T03:05:13.000Z</updated>
        <summary type="html"><![CDATA[<p>大厂面试总结，面到怀疑人生</p>
]]></summary>
        <content type="html"><![CDATA[<p>大厂面试总结，面到怀疑人生</p>
<!--more-->
<p>1、秒杀项目是完全你自己做的？<br>
2、能详细介绍一下接口优化吗？<br>
3、关于分布式session，你用redis存储，是用的持久化的吗，还是非持久化的？<br>
4、如果是非持久化的，那你的超时时间怎么讲？怎么设置的？<br>
5、那你给我讲一下redis吧，你对redis的理解<br>
6、你提到了主从复制、哨兵模式，能给我讲一下这一块吗？<br>
7、它是增强读还是增强读写？<br>
8、哨兵模式呢？<br>
9、我看你项目里也用到了消息系统，那在你的秒杀系统场景下，你是怎么用的消息的？<br>
10、如果说这台消息的broker（服务端）挂了呢？就是你刚才写的那个节点挂了呢，怎么办？<br>
11、消息一直重发，它的压力不会很大吗？<br>
12、对于这种问题，rabbitmq和kafka你知道它们是怎么解决的吗？<br>
13、刚才你提到把消息写到多个节点，那写多个节点怎么做呢？<br>
14、要保证多个节点的一致性，要怎么做呢？<br>
15、这个时候消费者端该如何消费呢？<br>
16、介绍一下nacos这一块吧<br>
17、刚刚你提到，zookeeper可用性差一点，那它可用性差在哪里？<br>
18、以两个机房为例，讲一讲nacos可用性为什么高、zookeeper为什么低？<br>
19、zookeeper是两个机房都不可用吗？<br>
20、zk其实是用的类似raft协议，你可以讲一下吗？或者就讲nacos的。<br>
21、断网以后，两个节点这边能接受写的请求吗？<br>
22、你能讲一下Distro协议吗？<br>
23、服务被A转发到对应的负责节点B，之后B会做哪些事情？<br>
24、B节点如何同步服务到其他节点？（什么策略）<br>
25、hash并取余是寻找server还是同步的时候做的？<br>
26、nacos配置管理这方面了解吗？<br>
27、nacos前一段时间发布了2.0版本，有关注吗?<br>
28、你有用过2.0吗？<br>
29、jvm里的一些GC的过程，分代收集之类的能介绍一下吗？<br>
30、平常会用哪类的垃圾回收器？配置会用哪个？<br>
31、那你知道CMS或者G1这种吗？<br>
32、CMS你没有用过对吧，你只是知道它的原理<br>
33、你有没有处理过full gc的问题？<br>
34、算法方面，分布式的一些算法你了解吗？比如说一致性hash算法。<br>
35、那一致性hash算法是解决什么问题呢？或者说它有什么优势？你刚才说的这么多，它这样做的原因是什么？36、关于TCP方面，你介绍一下三次握手呗？<br>
37、那关于tcp上面的滑动窗口，你有了解吗？<br>
38、专业方面做过的项目能介绍一下吗？不用说实现细节，可以讲一下团队协同之类的。<br>
39、为什么不继续做你现在的专业方向呢？为什么不继续做这个行业？<br>
40、你未来两到三年的职业规划是什么样的？<br>
41、现在不管是中间件还是云容器，都逐渐在往云原生在走，你对云原生是怎么理解的？<br>
42、刚刚说的可能更偏向云计算，那云原生呢？</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mysql 事务ACID特性]]></title>
        <id>https://rustysharon.github.io/post/mysql-shi-wu-acid/</id>
        <link href="https://rustysharon.github.io/post/mysql-shi-wu-acid/">
        </link>
        <updated>2021-08-19T02:33:33.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-acid">1. ACID</h1>
<p>在关系型数据库管理系统中，一个逻辑工作单元要成为事务，必须满足这 4 个特性，即所谓的 ACID:<br>
原子性(Atomicity)、一致性(Consistency)、隔离性(Isolation)和持久性(Durability)。</p>
<h2 id="11-原子性">1.1 原子性</h2>
<p>原子性:事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。 修改---》Buffer Pool修改---》刷盘。可能会有下面两种情况:</p>
<p>事务提交了，如果此时Buffer Pool的脏页没有刷盘，如何保证修改的数据生效? Redo 如果事务没提交，但是Buffer Pool的脏页刷盘了，如何保证不该存在的数据撤销?Undo</p>
<p>每一个写事务，都会修改BufferPool，从而产生相应的Redo/Undo日志，在Buffer Pool 中的页被刷到 磁盘之前，这些日志信息都会先写入到日志文件中，如果 Buffer Pool 中的脏页没有刷成功，此时数据 库挂了，那在数据库再次启动之后，可以通过 Redo 日志将其恢复出来，以保证脏页写的数据不会丢 失。如果脏页刷新成功，此时数据库挂了，就需要通过Undo来实现了。</p>
<h2 id="12-持久性">1.2 持久性</h2>
<p>持久性:指的是一个事务一旦提交，它对数据库中数据的改变就应该是永久性的，后续的操作或故障不<br>
应该对其有任何影响，不会丢失。<br>
如下图所示，一个“提交”动作触发的操作有:binlog落地、发送binlog、存储引擎提交、flush_logs， check_point、事务提交标记等。这些都是数据库保证其数据完整性、持久性的手段。<br>
<img src="https://rustysharon.github.io/post-images/1629686155495.png" alt="" loading="lazy"></p>
<p>MySQL的持久性也与WAL技术相关，redo log在系统Crash重启之类的情况时，可以修复数据，从而保 障事务的持久性。通过原子性可以保证逻辑上的持久性，通过存储引擎的数据刷盘可以保证物理上的持 久性。</p>
<h2 id="13-隔离性">1.3 隔离性</h2>
<p>隔离性:指的是一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对其他的并<br>
发事务是隔离的。<br>
InnoDB 支持的隔离性有 4 种，隔离性从低到高分别为:读未提交、读提交、可重复读、可串行化。锁<br>
和多版本控制(MVCC)技术就是用于保障隔离性的。</p>
<h2 id="14-一致性">1.4 一致性</h2>
<p>一致性:指的是事务开始之前和事务结束之后，数据库的完整性限制未被破坏。一致性包括两方面的内<br>
容，分别是约束一致性和数据一致性。<br>
约束一致性:创建表结构时所指定的外键、Check、唯一索引等约束，可惜在 MySQL 中不支持 Check 。 数据一致性:是一个综合性的规定，因为它是由原子性、持久性、隔离性共同保证的结果，而不是 单单依赖于某一种技术。<br>
一致性也可以理解为数据的完整性。数据的完整性是通过原子性、隔离性、持久性来保证的，而这3个 特性又是通过 Redo/Undo 来保证的。逻辑上的一致性，包括唯一索引、外键约束、check 约束，这属 于业务逻辑范畴。</p>
<p><img src="https://rustysharon.github.io/post-images/1629686232991.png" alt="" loading="lazy"><br>
<img src="https://rustysharon.github.io/post-images/1629686241230.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TCP 三次握手与四次挥手]]></title>
        <id>https://rustysharon.github.io/post/tcp-san-ci-wo-shou-yu-si-ci-hui-shou/</id>
        <link href="https://rustysharon.github.io/post/tcp-san-ci-wo-shou-yu-si-ci-hui-shou/">
        </link>
        <updated>2021-08-06T11:49:21.000Z</updated>
        <summary type="html"><![CDATA[<p>讨论 TCP 协议的设计原理，其中包括 TCP 协议的 三次握手、  四次挥手</p>
]]></summary>
        <content type="html"><![CDATA[<p>讨论 TCP 协议的设计原理，其中包括 TCP 协议的 三次握手、  四次挥手</p>
<!--more-->
<h1 id="1-三次挥手">1. 三次挥手</h1>
<p>会话是应用层的概念，连接是传输层的概念。<br>
1.最初客户端、服务器CLOSED状态，先是服务器主动监听某个端口，处于LISTEN状态</p>
<p>2.客户端发送连接请求报文，并将首部中的SYN标志位置为1，SYN即synchronized表示想<br>
要建立连接，并且初始化一个随机的序列号seq，保存在序列号字段中。发送完客户端处于<br>
SYN-SEND状态（同步已发送）。</p>
<p>3.服务器收到客户端的 请求报文后，服务端初始化随机的seq number ，并填入 TCP 首部序<br>
列号字段中，然后把客户端seq number加1放入确认号字段中。ack number 代表：期望对<br>
方下一次传过来的TCP数据部分的第一个字节的编号。 接着把 SYN和 ACK 标志位置为 1。<br>
然后把该报文发给客户端，，之后服务端处于 SYN-REV(同步已接收) 状态。</p>
<p>4.客户端收到服务器的确认连接请求后，发送最后一个应答报纸文，将ACK标志位置为1，<br>
seq number 为 （客seq + 1） ， ack number 为 （服seq+1）。发送完成后 客户端进入<br>
ESTABLISHED（连接已建立），服务器收到请求后也进入ESTABLISHED状态。<br>
<img src="https://rustysharon.github.io/post-images/1629287586797.png" alt="" loading="lazy"></p>
<h1 id="2-四次挥手">2. 四次挥手</h1>
<p>一个常见的关闭连接过程如下</p>
<p>1.当客户端没有待发送的数据时，它会向服务端发送 FIN 消息，发送消息后会进入 FIN_WAIT_1 状态；<br>
2.服务端接收到客户端的 FIN 消息后，会进入 CLOSE_WAIT 状态并向客户端发送 ACK 消息，客户端接收到 ACK 消息时会进入 FIN_WAIT_2 状态；<br>
3.当服务端没有待发送的数据时，服务端会向客户端发送 FIN 消息；<br>
4.客户端接收到 FIN 消息后，会进入 TIME_WAIT 状态并向服务端发送 ACK 消息，服务端收到后会进入 CLOSED 状态；<br>
5.客户端等待两个最大数据段生命周期（Maximum segment lifetime，MSL）2的时间后也会进入 CLOSED 状态；<br>
<img src="https://rustysharon.github.io/post-images/1629287719118.png" alt="" loading="lazy"></p>
<h1 id="time_wait-和-2msl">TIME_WAIT 和 2MSL</h1>
<p>MSL: Maximum segment lifetime</p>
<p>防止延迟的数据段被其他使用相同源地址、源端口、目的地址以及目的端口的 TCP 连接收到；<br>
保证 TCP 连接的远程被正确关闭，即等待被动关闭连接的一方收到 FIN 对应的 ACK 消息<br>
为什么等待2MSL，从TIME_WAIT到CLOSE？<br>
在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL(Maximum Segment Lifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。<br>
为了可靠地实现全双工连接的终止。必须确认Server接收到了该ACK<br>
允许老的重复分节在网络中消逝。可以防止本次连接中产生的数据包误传到下一次连接中</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mysql索引原理]]></title>
        <id>https://rustysharon.github.io/post/mysql-suo-yin-yuan-li/</id>
        <link href="https://rustysharon.github.io/post/mysql-suo-yin-yuan-li/">
        </link>
        <updated>2021-08-01T15:26:40.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-索引类型">1. 索引类型</h1>
<p>索引可以提升查询速度，会影响where查询，以及order by排序。MySQL索引类型如下:<br>
从索引存储结构划分:B Tree索引、Hash索引、FULLTEXT全文索引、R Tree索引 从应用层次划分:普通索引、唯一索引、主键索引、复合索引 从索引键值类型划分:主键索引、辅助索引(二级索引) 从数据存储和索引键值逻辑关系划分:聚集索引(聚簇索引)、非聚集索引(非聚簇索引)<br>
##1.1 普通索引<br>
这是最基本的索引类型，基于普通字段建立的索引，没有任何限制。 创建普通索引的方法如下:<br>
CREATE INDEX &lt;索引的名字&gt; ON tablename (字段名);<br>
ALTER TABLE tablename ADD INDEX [索引的名字] (字段名); CREATE TABLE tablename ( [...], INDEX [索引的名字] (字段名) );</p>
<p>##1.2 唯一索引</p>
<p>与&quot;普通索引&quot;类似，不同的就是:索引字段的值必须唯一，但允许有空值 。在创建或修改表时追加唯一<br>
约束，就会自动创建对应的唯一索引。<br>
创建唯一索引的方法如下:<br>
CREATE UNIQUE INDEX &lt;索引的名字&gt; ON tablename (字段名); ALTER TABLE tablename ADD UNIQUE INDEX [索引的名字] (字段名); CREATE TABLE tablename ( [...], UNIQUE [索引的名字] (字段名) ;</p>
<p>##1.3 主键索引<br>
它是一种特殊的唯一索引，不允许有空值。在创建或修改表时追加主键约束即可，每个表只能有一个主<br>
键。<br>
创建主键索引的方法如下:<br>
CREATE TABLE tablename ( [...], PRIMARY KEY (字段名) ); ALTER TABLE tablename ADD PRIMARY KEY (字段名);</p>
<p>##1.4 复合索引<br>
单一索引是指索引列为一列的情况，即新建索引的语句只实施在一列上;用户可以在多个列上建立索<br>
引，这种索引叫做组复合索引(组合索引)。复合索引可以代替多个单一索引，相比多个单一索引复合<br>
索引所需的开销更小。<br>
索引同时有两个概念叫做窄索引和宽索引，窄索引是指索引列为1-2列的索引，宽索引也就是索引列超 过2列的索引，设计索引的一个重要原则就是能用窄索引不用宽索引，因为窄索引往往比组合索引更有 效。<br>
创建组合索引的方法如下:<br>
CREATE INDEX &lt;索引的名字&gt; ON tablename (字段名1，字段名2...); ALTER TABLE tablename ADD INDEX [索引的名字] (字段名1，字段名2...);</p>
<p>CREATE TABLE tablename ( [...], INDEX [索引的名字] (字段名1，字段名2...) ); 复合索引使用注意事项:<br>
何时使用复合索引，要根据where条件建索引，注意不要过多使用索引，过多使用会对更新操作效 率有很大影响。 如果表已经建立了(col1，col2)，就没有必要再单独建立(col1);如果现在有(col1)索引，如果查 询需要col1和col2条件，可以建立(col1,col2)复合索引，对于查询有一定提高。</p>
<h2 id="15-全文索引">1.5 全文索引</h2>
<p>查询操作在数据量比较少时，可以使用like模糊查询，但是对于大量的文本数据检索，效率很低。如果 使用全文索引，查询速度会比like快很多倍。在MySQL 5.6 以前的版本，只有MyISAM存储引擎支持全 文索引，从MySQL 5.6开始MyISAM和InnoDB存储引擎均支持。<br>
创建全文索引的方法如下:<br>
CREATE FULLTEXT INDEX &lt;索引的名字&gt; ON tablename (字段名); ALTER TABLE tablename ADD FULLTEXT [索引的名字] (字段名); CREATE TABLE tablename ( [...], FULLTEXT KEY [索引的名字] (字段名) ;</p>
<p>#2.索引原理</p>
<h2 id="21hash结构">2.1Hash结构</h2>
<p>Hash底层实现是由Hash表来实现的，是根据键值 &lt;key,value&gt; 存储数据的结构。非常适合根据key查找<br>
value值，也就是单个key查询，或者说等值查询。其结构如下所示:<br>
<img src="https://rustysharon.github.io/post-images/1629646515529.png" alt="" loading="lazy"></p>
<p>从上面结构可以看出，Hash索引可以方便的提供等值查询，但是对于范围查询就需要全表扫描了。 Hash索引在MySQL 中Hash结构主要应用在Memory原生的Hash索引 、InnoDB 自适应哈希索引。<br>
InnoDB提供的自适应哈希索引功能强大，接下来重点描述下InnoDB 自适应哈希索引。<br>
InnoDB自适应哈希索引是为了提升查询效率，InnoDB存储引擎会监控表上各个索引页的查询，当 InnoDB注意到某些索引值访问非常频繁时，会在内存中基于B+Tree索引再创建一个哈希索引，使得内 存中的 B+Tree 索引具备哈希索引的功能，即能够快速定值访问频繁访问的索引页。<br>
InnoDB自适应哈希索引:在使用Hash索引访问时，一次性查找就能定位数据，等值查询效率要优于 B+Tree。<br>
自适应哈希索引的建立使得InnoDB存储引擎能自动根据索引页访问的频率和模式自动地为某些热点页 建立哈希索引来加速访问。另外InnoDB自适应哈希索引的功能，用户只能选择开启或关闭功能，无法 进行人工干涉。</p>
<h2 id="22-b-tree-结构">2.2 B+ Tree 结构</h2>
<p>MySQL数据库索引采用的是B+Tree结构，在B-Tree结构上做了优化改造。<br>
B-Tree结构<br>
索引值和data数据分布在整棵树结构中 每个节点可以存放多个索引值及对应的data数据 树节点中的多个索引值从左到右升序<br>
<img src="https://rustysharon.github.io/post-images/1629646607719.png" alt="" loading="lazy"><br>
B树的搜索:从根节点开始，对节点内的索引值序列采用二分法查找，如果命中就结束查找。没有 命中会进入子节点重复查找过程，直到所对应的的节点指针为空，或已经是叶子节点了才结束。<br>
B+Tree结构：<br>
<img src="https://rustysharon.github.io/post-images/1629646618649.png" alt="" loading="lazy"></p>
<p>相比B树，B+树进行范围查找时，只需要查找定位两个节点的索引值，然后利用叶子节点的指针进 行遍历即可。而B树需要遍历范围内所有的节点和数据，显然B+Tree效率高。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redis缓存穿透解决思路]]></title>
        <id>https://rustysharon.github.io/post/redis-huan-cun-chuan-tou-jie-jue-si-lu/</id>
        <link href="https://rustysharon.github.io/post/redis-huan-cun-chuan-tou-jie-jue-si-lu/">
        </link>
        <updated>2021-07-12T06:22:26.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-缓存穿透">1. 缓存穿透</h1>
<p>一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找(比如 DB)。<br>
缓存穿透是指在高并发下查询key不存在的数据，会穿过缓存查询数据库。导致数据库压力过大而宕机 解决方案:<br>
对查询结果为空的情况也进行缓存，缓存时间(ttl)设置短一点，或者该key对应的数据insert了 之后清理缓存。 问题:缓存太多空值占用了更多的空间<br>
使用布隆过滤器。在缓存之前在加一层布隆过滤器，在查询的时候先去布隆过滤器查询 key 是否 存在，如果不存在就直接返回，存在再查缓存和DB。<br>
<img src="https://rustysharon.github.io/post-images/1629267832269.png" alt="" loading="lazy"></p>
<p>布隆过滤器(Bloom Filter)是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机 hash映射函数。<br>
布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般<br>
的算法。<br>
<img src="https://rustysharon.github.io/post-images/1629267965133.png" alt="" loading="lazy"><br>
布隆过滤器的原理是，当一个元素被加入集合时，通过K个Hash函数将这个元素映射成一个数组中的K 个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就(大约)知道集合中有没有它了:如 果这些点有任何一个0，则被检元素一定不在;如果都是1，则被检元素很可能在。这就是布隆过滤器的 基本思想。</p>
<p>布隆过滤器主要有两个缺陷：</p>
<p>1.它在判断元素是否在集合中时是有一定错误几率的，比如它会把不是集合中的元素判断为处在集合中；</p>
<p>2.不支持删除元素。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[端点科技三面面经 offer]]></title>
        <id>https://rustysharon.github.io/post/duan-dian-ke-ji-san-mian-mian-jing-offer/</id>
        <link href="https://rustysharon.github.io/post/duan-dian-ke-ji-san-mian-mian-jing-offer/">
        </link>
        <updated>2021-07-10T06:37:27.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-一面">1. 一面</h1>
<ol>
<li>hashmap 底层结构 、 put过程、扩容</li>
<li>ArrayList 与 LinkedList</li>
<li>聚簇索引 与非聚簇索引</li>
<li>回表</li>
<li>排序各种时间复杂度</li>
<li>Synchoronized、volatile关键字</li>
<li>Spring Boot IOC 与Aop</li>
<li>redis 数据类型、持久化</li>
<li>jvm<br>
10.线程池参数</li>
</ol>
<p>#2. 二面</p>
<ol>
<li>项目登录</li>
<li>秒杀流程</li>
<li>如何解决超卖</li>
<li>消息如何不丢失的、重复消费</li>
<li>分布式锁</li>
<li>单例模式</li>
<li>grpc 、protobuf</li>
<li>netty nio</li>
<li>reids</li>
</ol>
<p>#3. hr<br>
问问题<br>
1.最快入职日期？<br>
2. 看什么书<br>
3.爱好</p>
<p>反问：<br>
1.业务<br>
2. 技术栈</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[垃圾回收机制及算法]]></title>
        <id>https://rustysharon.github.io/post/la-ji-hui-shou-ji-zhi-ji-suan-fa/</id>
        <link href="https://rustysharon.github.io/post/la-ji-hui-shou-ji-zhi-ji-suan-fa/">
        </link>
        <updated>2021-06-20T15:16:15.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-对象是否已死">1. 对象是否已死</h1>
<figure data-type="image" tabindex="1"><img src="https://rustysharon.github.io/post-images/1629645819344.png" alt="" loading="lazy"></figure>
<h2 id="11引用计数算法">1.1引用计数算法</h2>
<p>引用计数算法可以这样实现:给每个创建的对象添加一个引用计数器，每当此对象被某个地方引用时，计数值+1， 引用失效时-1，所以当计数值为0时表示对象已经不能被使用。引用计数算法大多数情况下是个比较不错的算法， 简单直接，也有一些著名的应用案例但是对于Java虚拟机来说，并不是一个好的选择，因为它很难解决对象直接相 互循环引用的问题。</p>
<h2 id="12可达性分析算法">1.2可达性分析算法</h2>
<p>在主流的商用程序语言如Java、C#等的主流实现中，都是通过可达性分析(Reachability Analysis)来判断对象是否存 活的。此算法的基本思路就是通过一系列的“GC Roots”的对象作为起始点，从起始点开始向下搜索到对象的路径。 搜索所经过的路径称为引用链(Reference Chain)，当一个对象到任何GC Roots都没有引用链时，则表明对象“不可 达” ，即该对象是不可用的。<br>
<img src="https://rustysharon.github.io/post-images/1629645488344.png" alt="" loading="lazy"></p>
<p>在Java语言中，可作为GC Roots的对象包括下面几种:<br>
栈帧中的局部变量表中的reference引用所引用的对象<br>
方法区中static静态引用的对象<br>
方法区中final常量引用的对象<br>
本地方法栈中JNI(Native方法)引用的对象<br>
Java虚拟机内部的引用， 如基本数据类型对应的Class对象， 一些常驻的异常对象(比如 NullPointExcepiton、 OutOfMemoryError) 等， 还有系统类加载器。<br>
所有被同步锁(synchronized关键字) 持有的对象。</p>
<h1 id="2-垃圾回收算法">2 垃圾回收算法</h1>
<h2 id="21-标记-清除">2.1 标记-清除</h2>
<p>最早出现也是最基础的垃圾收集算法是“标记-清除”(Mark-Sweep) 算法， 在1960年由Lisp之父 John McCarthy所 提出。 如它的名字一样， 算法分为“标记”和“清除”两个阶段: 首先标记出所有需要回 收的对象， 在标记完成后， 统一回收掉所有被标记的对象， 也可以反过来， 标记存活的对象， 统一回 收所有未被标记的对象。<br>
标记过程就是对象是否属于垃圾的判定过程， 这在前一节讲述垃圾对象标记 判定算法时其实已经介绍过了。 之所 以说它是最基础的收集算法， 是因为后续的收集算法大多都是以标记-清除算法为基础， 对其 缺点进行改进而得到 的。<br>
<img src="https://rustysharon.github.io/post-images/1629645614015.png" alt="" loading="lazy"></p>
<h2 id="22-标记-复制">2.2 标记-复制</h2>
<p>了解决标记-清除算法面对大量可回收对象时执行效率低的问题， 1969年Fenichel提出了一种称为“半区复 制”(Semispace Copying) 的垃圾收集算法， 它将可用 内存按容量划分为大小相等的两块， 每次只使用其中的一 块。 当这一块的内存用完了， 就将还存活着 的对象复制到另外一块上面， 然后再把已使用过的内存空间一次清理 掉。 如果内存中多数对象都是存活的， 这种算法将会产生大量的内存间复制的开销， 但对于多数对象都是可回收 的情况， 算法需要复制的就是占少数的存活对象， 而且每次都是针对整个半区进行内存回收， 分配内存时也就不 用考虑有空间碎片的复杂情况， 只要移动堆顶指针， 按顺序分配即可.</p>
<figure data-type="image" tabindex="2"><img src="https://rustysharon.github.io/post-images/1629645677716.png" alt="" loading="lazy"></figure>
<h2 id="23-标记-整理">2.3 标记-整理</h2>
<p>标记-复制算法在对象存活率较高时就要进行较多的复制操作， 效率将会降低。 更关键的是， 如果不想浪费50%的 空间， 就需要有额外的空间进行分配担保， 以应对被使用的内存中所有对象都100%存活的极端情况， 所以在老年 代一般不能直接选用这种算法。<br>
针对老年代对象的存亡特征， 1974年Edward Lueders提出了另外一种有针对性的“标记-整 理”(Mark-Compact) 算 法， 其中的标记过程仍然与“标记-清除”算法一样， 但后续步骤不是直接对可回收对象进行清理， 而是让所有存活 的对象都向内存空间一端移动， 然后直接清理掉边界以外的内存 。</p>
<p>标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法， 而后者是移动式的。 是否移动 回收后的存活对象是一项优缺点并存的风险决策:</p>
<figure data-type="image" tabindex="3"><img src="https://rustysharon.github.io/post-images/1629645686120.png" alt="" loading="lazy"></figure>
<h1 id="3-垃圾回收器">3 垃圾回收器</h1>
<h2 id="31-cms">3.1 CMS</h2>
<p>CMS(concurrent mark sweep)是以获取最短垃圾收集停顿时间为目标的收集器，CMS收集器的关注点尽可能缩短 垃圾收集时用户线程的停顿时间,停顿时间越短就越适合与用户交互的程序,目前很大一部分的java应用几种在互联网 的B/S系统服务器上，这类应用尤其注重服务器的响应速度，系统停顿时间最短，给用户带来良好的体验，CMS收 集器使用的算法是标记-清除算法实现的;<br>
4.7.2 CMS垃圾收集过程 整个过程分4个步骤: 1)初始标记<br>
2) 并发标记<br>
3) 重新标记<br>
4) 并发清除<br>
其中 初始标记 和 重新标记 都需要stopTheWorld</p>
<p>CMS整个过程比之前的收集器要复杂,整个过程分为4个阶段即、初始标记 并发标记 、重新标记、并发清除<br>
初始标记(Initial-Mark)阶段:这个阶段程序所有的工作线程都将会因为&quot;Stop-the-Wold&quot;机制而出现短暂的的 暂停,这个阶段的主要任务标记处GC Roots 能够关联到的对象.一旦标记完成后就恢复之前被暂停的的所有应 用。 由于直接关联对象比较小，所以这里的操作速度非常快。<br>
并发标记(Concurrent-Mark)阶段:从GC Roots的直接关联对象开始遍历整个对象图的过程,这个过程耗时较 长,但是不需要暂停用户线程, 用户线程可以与垃圾回收器一起运行。 重新标记(Remark)阶段:由于并发标记阶段，程序的工作线程会和垃圾收集线程同时运行或者交叉运行， 因此，为了修正并发标记期间因为用户继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段 的停顿时间通常比初始标记阶段长一些，但也远比并发标记阶段时间短。 清除并发(Concurrent-Sweep)阶段: 此阶段清理删除掉标记判断已经死亡的对象,并释放内存空间。由于不需 要移动存活对象，所以这个阶段可以与用户线程同时并发运行。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMQ 入门二 消息]]></title>
        <id>https://rustysharon.github.io/post/rocketmq-ru-men-er-xiao-xi/</id>
        <link href="https://rustysharon.github.io/post/rocketmq-ru-men-er-xiao-xi/">
        </link>
        <updated>2021-05-11T02:25:15.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-环境搭建">1. 环境搭建</h1>
<ol>
<li>下载rocketmq</li>
</ol>
<blockquote>
<p>#下载<br>
wgethttps://archive.apache.org/dist/rocketmq/4.5.1/ocketmq-all4.5.1-bin-release.zip</p>
</blockquote>
<h1 id="2-启动nameserver">2. 启动NameServer</h1>
<pre><code class="language-shell"># 1.启动NameServer
mqnamesrv
# 2.查看启动日志
tail -f ~/logs/rocketmqlogs/namesrv.log
</code></pre>
<h1 id="3启动broker">3.启动Broker</h1>
<pre><code class="language-shell"># 1.启动Broker
mqbroker -n localhost:9876
# 2.查看启动日志
tail -f ~/logs/rocketmqlogs/broker.log
</code></pre>
<h1 id="4rocketmq相关api使用">4.RocketMQ相关API使用</h1>
<p>DefaultMQProducer 生产者的默认实现 生产消息分同步发送和异步发送 DefaultMQConsumer:消费者的默认实现 消息的拉取和消息的推送</p>
<p>消息生产者：</p>
<pre><code class="language-java">public class MyProducer {
    public static void main(String[] args) throws
UnsupportedEncodingException, InterruptedException, RemotingException,
MQClientException, MQBrokerException {
// 在实例化生产者的同时，指定了生产组名称
        DefaultMQProducer producer = new
DefaultMQProducer(&quot;myproducer_grp_01&quot;);
// 指定NameServer的地址 producer.setNamesrvAddr(&quot;node1:9876&quot;);
// 对生产者进行初始化，然后就可以使用了 producer.start();
MyAsyncProducer.java

// 创建消息，第一个参数是主题名称，第二个参数是消息内容 Message message = new Message(
                &quot;tp_demo_01&quot;,
                &quot;hello lagou 01&quot;.getBytes(RemotingHelper.DEFAULT_CHARSET)
);
// 发送消息
final SendResult result = producer.send(message); System.out.println(result);
// 关闭生产者
        producer.shutdown();
    }
}
</code></pre>
<p>消息消费者：</p>
<pre><code class="language-java">public class MyAsyncProducer {
    public static void main(String[] args) throws MQClientException,
UnsupportedEncodingException, RemotingException, InterruptedException { // 实例化生产者，并指定生产组名称
        DefaultMQProducer producer = new
DefaultMQProducer(&quot;producer_grp_01&quot;);
// 指定nameserver的地址 producer.setNamesrvAddr(&quot;node1:9876&quot;);
// 初始化生产者 producer.start();
        for (int i = 0; i &lt; 100; i++) {
            Message message = new Message(
                    &quot;tp_demo_02&quot;,
                    (&quot;hello lagou &quot; + i).getBytes(&quot;utf-8&quot;)
);
// 消息的异步发送
producer.send(message, new SendCallback() {
@Override
                public void onSuccess(SendResult sendResult) {
MyPullConsumer.java

System.out.println(&quot;发送成功:&quot; + sendResult); }
@Override
public void onException(Throwable throwable) { System.out.println(&quot;发送失败:&quot; +
throwable.getMessage());
                }
}); }
// 由于是异步发送消息，上面循环结束之后，消息可能还没收到broker的响应 // 如果不sleep一会儿，就报错
Thread.sleep(10_000);
// 关闭生产者
        producer.shutdown();
    }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMQ 入门一]]></title>
        <id>https://rustysharon.github.io/post/rocketmq-ru-men-yi/</id>
        <link href="https://rustysharon.github.io/post/rocketmq-ru-men-yi/">
        </link>
        <updated>2021-03-08T16:02:49.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-消息队列应用场景">1. 消息队列应用场景</h1>
<p>应用解耦：<br>
系统的耦合性越高，容错性就越低。以电商应用为例，用户创建订单后，如果耦合调用库存系统、<br>
物流系统、支付系统，任何一个子系统出了故障或者因为升级等原因暂时不可用，都会造成下单操作异<br>
常，影响用户使用体验。<br>
流量削峰：<br>
应用系统如果遇到系统请求流量的瞬间猛增，有可能会将系统压垮。有了消息队列可以将大量请求<br>
缓存起来，分散到很长一段时间处理，这样可以大大提到系统的稳定性和用户体验。<br>
举例:业务系统正常时段的QPS如果是1000，流量最高峰是10000，为了应对流量高峰配置高性能 的服务器显然不划算，这时可以使用消息队列对峰值流量削峰<br>
数据分发：<br>
通过消息队列可以让数据在多个系统之间进行流通。数据的产生方不需要关心谁来使用数据，只需<br>
要将数据发送到消息队列，数据使用方直接在消息队列中直接获取数据即可</p>
<h1 id="2基本架构">2.基本架构</h1>
<p>RocketMQ的角色介绍<br>
Producer:消息的发送者;举例:发信者<br>
Consumer:消息接收者;举例:收信者<br>
Broker:暂存和传输消息;举例:邮局 NameServer:管理Broker;举例:各个邮局的管理机构 Topic:区分消息的种类;一个发送者可以发送消息给一个或者多个Topic;一个消息的接收者 可以订阅一个或者多个Topic消息<br>
Message Queue:相当于是Topic的分区;用于并行发送和接收消息<br>
<img src="https://rustysharon.github.io/post-images/1629648309313.png" alt="" loading="lazy"></p>
<p>执行流程:</p>
<ol>
<li>启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上 来，相当于一个路由控制中心。</li>
<li>Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前 Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic 跟Broker的映射关系。</li>
<li>收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在 发送消息时自动创建Topic。</li>
<li>Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从 NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列， 然后与队列所在的Broker建立长连接从而向Broker发消息。</li>
<li>Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在 哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Netty 粘包与拆包]]></title>
        <id>https://rustysharon.github.io/post/netty-nian-bao-yu/</id>
        <link href="https://rustysharon.github.io/post/netty-nian-bao-yu/">
        </link>
        <updated>2021-03-04T14:30:29.000Z</updated>
        <summary type="html"><![CDATA[<p>粘包和拆包是TCP网络编程中不可避免的。<br>
...</p>
]]></summary>
        <content type="html"><![CDATA[<p>粘包和拆包是TCP网络编程中不可避免的。<br>
...</p>
<!-- more -->
<p>TCP粘包和拆包产生的原因:<br>
数据从发送方到接收方需要经过操作系统的缓冲区，而造成粘包和拆包的主要原因就在这个缓冲区<br>
上。粘包可以理解为缓冲区数据堆积，导致多个请求数据粘在一起，而拆包可以理解为发送的数据大于<br>
缓冲区，进行拆分处理。</p>
<figure data-type="image" tabindex="1"><img src="https://rustysharon.github.io/post-images/1628174000420.png" alt="" loading="lazy"></figure>
<p>Netty提供了4种解码器来解决，分别如下:<br>
1.固定长度的拆包器 FixedLengthFrameDecoder，每个应用层数据包的都拆分成都是固定长度 的大小</p>
<p>2.行拆包器 LineBasedFrameDecoder，每个应用层数据包，都以换行符作为分隔符，进行分割 拆分</p>
<p>3.分隔符拆包器 DelimiterBasedFrameDecoder，每个应用层数据包，都通过自定义的分隔 符，进行分割拆分</p>
<p>4.基于数据包长度的拆包器 LengthFieldBasedFrameDecoder，将应用层数据包的长度，作为 接收端应用层数据包的拆分依据。按照应用层数据包的大小，拆包。这个拆包器，有一个要 求，就是应用层协议中包含数据包的长度</p>
]]></content>
    </entry>
</feed>